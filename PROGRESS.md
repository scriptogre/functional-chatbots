## Progress

Follow the progress table below to see the different milestones / branches for this workshop.

| Branch                      | Status | Type     | Description                                                |
|-----------------------------|-------|----------|------------------------------------------------------------|
| `0-start-here`              | ✅     | Start    | Set up the project, get familiar with the codebase.        |
| `1-integrate-llm-task`      | ✅      | Task     | Integrate the LLM within the htmx chat.                    |
| `2-integrate-llm-solution`  | ⏳     | **Solution** |                                                            |
| `3-client-events-task`      | ⏭     | Task     | Enable the LLM to trigger dark mode & fullscreen mode.     |
| `4-client-events-solution`  | ⏭     | Solution |                                                            |
| `5-server-functions-task`     | ⏭     | Task     | Enable the LLM to create, update, and delete pizza orders. |
| `6-server-functions-solution` | ⏭     | Solution |                                                            |

To switch to another branch, use `git switch`.

**Note**: You don't need to run `docker compose up` for each branch. Just switch branch & refresh browser.

---

### ~~Current Task: Integrate the LLM within the htmx chat.~~

**Congratulations!** You've completed the first task.

<img src="https://cdn.jsdelivr.net/gh/scriptogre/functional-chatbots-assets@main/2-integrate-llm-solution/assistant_responds_haikus.gif" width=800 alt="First Solution"/>

You now have a basic chat application that can generate responses using LLama 3.

---

# Next Steps

But that's not too impressive, is it? Let's see what else we can do.

Switch to the `3-client-events-task` branch to see the next task.
```bash
git switch 3-client-events-task
```